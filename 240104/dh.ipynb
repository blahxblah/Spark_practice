{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "131072x1 화면 크기가 잘못됐습니다. 문제가 예상됩니다\n",
      "24/01/05 02:23:49 WARN Utils: Your hostname, KJH-DESKTOP resolves to a loopback address: 127.0.1.1; using 192.168.69.220 instead (on interface eth0)\n",
      "24/01/05 02:23:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/05 02:23:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/05 02:23:51 WARN RapidsPluginUtils: RAPIDS Accelerator 23.12.0 using cudf 23.12.0.\n",
      "24/01/05 02:23:51 WARN RapidsPluginUtils: spark.rapids.sql.multiThreadedRead.numThreads is set to 20.\n",
      "24/01/05 02:23:51 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "24/01/05 02:23:51 WARN RapidsPluginUtils: spark.rapids.sql.explain is set to `NOT_ON_GPU`. Set it to 'NONE' to suppress the diagnostics logging about the query placement on the GPU.\n"
     ]
    }
   ],
   "source": [
    "# 스파크 세션 생성\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SynapseML : 마이크로소프트에서 만든 스파크용 ML 라이브러리\n",
    "# https://microsoft.github.io/SynapseML/\n",
    "# config에서 synapseml 모듈 불러오기\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark study - 240104\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/05 02:23:53 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "    ! <BatchEvalPythonExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.python.BatchEvalPythonExec\n",
      "      @Expression <PythonUDF> udf_func(field1#0L, field2#1L, field3#2L, field4#3L)#10 could not block GPU acceleration\n",
      "        @Expression <AttributeReference> field1#0L could run on GPU\n",
      "        @Expression <AttributeReference> field2#1L could run on GPU\n",
      "        @Expression <AttributeReference> field3#2L could run on GPU\n",
      "        @Expression <AttributeReference> field4#3L could run on GPU\n",
      "      @Expression <AttributeReference> pythonUDF0#36 could run on GPU\n",
      "        ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "          @Expression <AttributeReference> field1#0L could run on GPU\n",
      "          @Expression <AttributeReference> field2#1L could run on GPU\n",
      "          @Expression <AttributeReference> field3#2L could run on GPU\n",
      "          @Expression <AttributeReference> field4#3L could run on GPU\n",
      "          @Expression <AttributeReference> label#4 could run on GPU\n",
      "\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+-----+------------+\n",
      "|field1|field2|field3|field4|label|  prediction|\n",
      "+------+------+------+------+-----+------------+\n",
      "|     1|     2|     3|     4|  5.0|-0.513734043|\n",
      "|     2|     3|     4|     5|  6.0|-9.016757011|\n",
      "|     3|     4|     5|     6|  7.0|-7.097997665|\n",
      "+------+------+------+------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FieldAwareFactorizationMachine(nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super(FieldAwareFactorizationMachine, self).__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(field_dim, embed_dim) for field_dim in field_dims\n",
    "        ])\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = [embedding(x[:, i]) for i, embedding in enumerate(self.embeddings)]\n",
    "        interactions = torch.sum(torch.stack(embeddings), dim=0)\n",
    "        embeddings_squared = torch.stack([embedding**2 for embedding in embeddings])\n",
    "        ffm_term = torch.sum(interactions**2 - torch.sum(embeddings_squared, dim=0), dim=1, keepdim=True)\n",
    "        return ffm_term + self.bias\n",
    "\n",
    "def torch_model_udf(model):\n",
    "    def udf_func(*args):\n",
    "        with torch.no_grad():\n",
    "            input_tensor = torch.tensor([args], dtype=torch.long)\n",
    "            output_tensor = model(input_tensor)\n",
    "            return float(output_tensor.numpy().item())\n",
    "\n",
    "    return udf(udf_func, DoubleType())\n",
    "\n",
    "data = [(1, 2, 3, 4, 5.0), (2, 3, 4, 5, 6.0), (3, 4, 5, 6, 7.0)]\n",
    "columns = [\"field1\", \"field2\", \"field3\", \"field4\", \"label\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "field_dims = [5, 6, 7, 8] \n",
    "embed_dim = 5\n",
    "ffm_model = FieldAwareFactorizationMachine(field_dims, embed_dim)\n",
    "udf_model = torch_model_udf(ffm_model)\n",
    "\n",
    "result_df = df.withColumn(\"prediction\", udf_model(*columns[:-1]))\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kjh-4pV5HF-C",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

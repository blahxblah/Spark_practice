{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "131072x1 화면 크기가 잘못됐습니다. 문제가 예상됩니다\n",
      "24/01/06 23:32:55 WARN Utils: Your hostname, KJH-DESKTOP resolves to a loopback address: 127.0.1.1; using 192.168.69.220 instead (on interface eth0)\n",
      "24/01/06 23:32:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/06 23:32:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/06 23:32:56 WARN RapidsPluginUtils: RAPIDS Accelerator 23.12.0 using cudf 23.12.0.\n",
      "24/01/06 23:32:56 WARN RapidsPluginUtils: spark.rapids.sql.multiThreadedRead.numThreads is set to 20.\n",
      "24/01/06 23:32:56 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "0\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "1\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "2\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/06 23:33:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/01/06 23:33:22 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers:\n",
      "[ 3.18714286e+01  5.48571429e+00  5.57142857e-01  4.00000000e+00\n",
      "  1.25724286e+02  2.65791143e-01  6.26900000e-01  7.36600000e-01\n",
      "  8.28374143e-04  1.67651429e-01 -4.41340000e+00  4.72457143e-02\n",
      "  5.05185714e-01]\n",
      "[ 7.23214286e+01  5.25000000e+00  5.71428571e-01  4.00000000e+00\n",
      "  1.36270750e+02  1.20300000e-01  7.49714286e-01  7.80964286e-01\n",
      "  2.98342857e-04  1.31807143e-01 -4.60378571e+00  7.30321429e-02\n",
      "  6.91214286e-01]\n",
      "[ 5.06785714e+01  6.10714286e+00  5.35714286e-01  4.00000000e+00\n",
      "  9.71753214e+01  2.98810714e-01  6.66071429e-01  7.59928571e-01\n",
      "  1.20296429e-05  1.87292857e-01 -4.12535714e+00  9.61642857e-02\n",
      "  5.77292857e-01]\n",
      "[ 3.54117647e+01  5.47058824e+00  7.64705882e-01  3.82352941e+00\n",
      "  1.61670471e+02  1.64845059e-01  4.89176471e-01  7.78117647e-01\n",
      "  9.30452941e-04  1.62670588e-01 -4.40194118e+00  7.25294118e-02\n",
      "  5.58235294e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---+----+--------------+-------+------------+------------+------+----------------+--------+--------+-----------+-------+--------------------+--------------------+----------+\n",
      "|                  id|popularity|key|mode|time_signature|  tempo|acousticness|danceability|energy|instrumentalness|liveness|loudness|speechiness|valence|            features|     scaled_features|prediction|\n",
      "+--------------------+----------+---+----+--------------+-------+------------+------------+------+----------------+--------+--------+-----------+-------+--------------------+--------------------+----------+\n",
      "|1SJS8NiUV1aI6Gxhx...|         2|  0|   1|             4|115.009|       0.367|       0.803| 0.672|         4.55E-4|   0.229|  -4.486|     0.0348|  0.962|[2.0,0.0,1.0,4.0,...|[0.02,0.0,1.0,0.8...|         0|\n",
      "|6L2BtMXLBpBCUiivS...|         5|  0|   1|             3| 77.528|     0.00931|       0.542| 0.542|         6.77E-5|   0.146|  -6.951|     0.0233|  0.384|[5.0,0.0,1.0,3.0,...|[0.05,0.0,1.0,0.6...|         2|\n",
      "|5KIPX8znuP5bop6EM...|         0|  7|   1|             3|131.383|       0.987|       0.377| 0.309|          7.0E-5|   0.536|  -11.51|     0.0661|  0.195|[0.0,7.0,1.0,3.0,...|[0.0,0.6363636363...|         0|\n",
      "|1kJN1aLlPNA73KnFo...|         0|  3|   1|             4| 98.352|       0.911|       0.517| 0.384|           0.917|   0.598| -15.492|     0.0312|  0.307|[0.0,3.0,1.0,4.0,...|[0.0,0.2727272727...|         0|\n",
      "|1DV7jVNRymi3btcvj...|         0|  4|   0|             1|103.829|       0.995|       0.234| 0.129|           0.133|    0.11| -20.247|     0.0451| 0.0314|[0.0,4.0,0.0,1.0,...|[0.0,0.3636363636...|         0|\n",
      "|7w9jvhwSn2fYjrKwJ...|         7|  9|   1|             4|100.007|      0.0354|        0.53| 0.655|         3.24E-6|   0.164|   -7.48|     0.0312|   0.33|[7.0,9.0,1.0,4.0,...|[0.07,0.818181818...|         0|\n",
      "|1vUy59eMV0zRtBadE...|         5|  7|   1|             4|129.989|     9.78E-4|       0.803| 0.744|           0.898|   0.101|  -8.122|     0.0561|   0.66|[5.0,7.0,1.0,4.0,...|[0.05,0.636363636...|         0|\n",
      "|4sfOiMFo7fwp73R8b...|         2|  4|   0|             4| 93.015|       0.329|       0.633| 0.646|         5.34E-4|   0.274|  -6.412|     0.0479|  0.393|[2.0,4.0,0.0,4.0,...|[0.02,0.363636363...|         0|\n",
      "|27XUK67mYFoklYvTF...|        24|  1|   0|             4| 104.61|      0.0876|       0.524|  0.82|           0.419|   0.116|  -8.262|      0.131|   0.74|[24.0,1.0,0.0,4.0...|[0.24,0.090909090...|         0|\n",
      "|1muZbLyv7kaPnZ1yB...|         7|  4|   0|             3|135.954|      0.0143|       0.589| 0.828|           0.895|  0.0898|  -9.231|     0.0282|  0.727|[7.0,4.0,0.0,3.0,...|[0.07,0.363636363...|         0|\n",
      "|3afC5P1VWTD0F8Dvx...|        12|  7|   0|             4| 79.079|       0.317|       0.648| 0.918|             0.0|  0.0897|  -4.701|      0.366|  0.883|[12.0,7.0,0.0,4.0...|[0.12,0.636363636...|         2|\n",
      "|1z9VoMlNu4d5W0B9O...|         0|  2|   1|             4|194.264|      0.0103|       0.387| 0.876|          1.2E-4|   0.389|  -7.595|      0.513|  0.418|[0.0,2.0,1.0,4.0,...|[0.0,0.1818181818...|         3|\n",
      "|6g12MKnYzHMyFRGWu...|        19|  8|   1|             4|105.734|       0.571|       0.733| 0.249|             0.0|   0.113|   -9.34|     0.0835|  0.763|[19.0,8.0,1.0,4.0...|[0.19,0.727272727...|         0|\n",
      "|6O80gBEAaNbk5bAcn...|         2|  5|   1|             4|100.625|       0.963|       0.562| 0.365|           0.855|   0.296| -15.546|     0.0511|  0.914|[2.0,5.0,1.0,4.0,...|[0.02,0.454545454...|         0|\n",
      "|48N0RRR6Kl3A20WDd...|         0| 11|   0|             3|104.454|        0.97|       0.329| 0.137|           0.907|   0.162| -20.016|     0.0448| 0.0696|[0.0,11.0,0.0,3.0...|[0.0,1.0,0.0,0.60...|         0|\n",
      "|38CFyAfRU0jQ8qLRh...|         2|  3|   1|             3|  69.86|       0.984|       0.302|0.0116|           0.934|  0.0931| -24.454|     0.0381|  0.314|[2.0,3.0,1.0,3.0,...|[0.02,0.272727272...|         2|\n",
      "|2SCeM3adGOFqrsiO6...|         1|  1|   0|             4| 81.577|       0.986|        0.63| 0.224|           0.627|  0.0997| -22.349|     0.0816|  0.183|[1.0,1.0,0.0,4.0,...|[0.01,0.090909090...|         2|\n",
      "|2j2r5vNFr6uKbKcYp...|         0|  5|   1|             4| 74.631|       0.995|      0.0933|0.0299|           0.913|  0.0928| -28.083|      0.057|  0.034|[0.0,5.0,1.0,4.0,...|[0.0,0.4545454545...|         2|\n",
      "|1tYEih4xAu6BwRqrn...|        10|  2|   1|             4|112.227|       0.688|       0.631| 0.415|             0.0|   0.749| -11.432|     0.0368|  0.754|[10.0,2.0,1.0,4.0...|[0.1,0.1818181818...|         0|\n",
      "|5S58GhvZQ1VNpKAkv...|         2|  3|   1|             4| 108.04|       0.409|       0.375| 0.422|             0.0|   0.103|  -6.567|     0.0319|  0.251|[2.0,3.0,1.0,4.0,...|[0.02,0.272727272...|         0|\n",
      "+--------------------+----------+---+----+--------------+-------+------------+------------+------+----------------+--------+--------+-----------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[0.0,2.0,1.0,4.0,...|         0|\n",
      "|[0.0,1.0,1.0,5.0,...|         0|\n",
      "|[32.0,0.0,0.0,4.0...|         2|\n",
      "|[2.0,1.0,1.0,3.0,...|         0|\n",
      "|[21.0,10.0,1.0,4....|         0|\n",
      "|[0.0,7.0,1.0,4.0,...|         0|\n",
      "|[0.0,11.0,0.0,4.0...|         0|\n",
      "|[0.0,5.0,1.0,4.0,...|         0|\n",
      "|[1.0,0.0,1.0,4.0,...|         0|\n",
      "|[1.0,5.0,1.0,4.0,...|         3|\n",
      "|[0.0,6.0,0.0,4.0,...|         0|\n",
      "|[0.0,9.0,1.0,4.0,...|         2|\n",
      "|[2.0,7.0,0.0,3.0,...|         2|\n",
      "|[0.0,1.0,0.0,4.0,...|         0|\n",
      "|[0.0,10.0,1.0,4.0...|         2|\n",
      "|[0.0,2.0,1.0,4.0,...|         0|\n",
      "|[3.0,2.0,0.0,3.0,...|         3|\n",
      "|[0.0,8.0,1.0,4.0,...|         0|\n",
      "|[16.0,9.0,1.0,4.0...|         0|\n",
      "|[6.0,7.0,1.0,4.0,...|         3|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DEFINE FUNCTIONS <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "def get_access_token(client_id:str, client_sc:str):\n",
    "    import requests\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    }\n",
    "    data = f'grant_type=client_credentials&client_id={client_id}&client_secret={client_sc}'.encode()\n",
    "    response = requests.post('https://accounts.spotify.com/api/token', headers=headers, data=data).json()\n",
    "    access_token = response['access_token']\n",
    "\n",
    "    return access_token\n",
    "\n",
    "def get_response(access_token:str, endpoint:str, params:dict=None):\n",
    "    import requests, json\n",
    "\n",
    "    url = f\"https://api.spotify.com/v1/{endpoint}\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}',\n",
    "    }\n",
    "\n",
    "    if params != None:\n",
    "        response = requests.get(url=url, params=params, headers=headers)\n",
    "    else:\n",
    "        response = requests.get(url=url, headers=headers)\n",
    "    print(response)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            return data\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            raise ValueError(f\"API Server Error - {endpoint} - Invalid JSON content in response: {response.text}\")\n",
    "    else:\n",
    "        raise ValueError(f\"API Server Error - {endpoint} - Non-200 status code received: {response.status_code}\")\n",
    "    \n",
    "\n",
    "def post_response(access_token:str, endpoint:str, data:dict=None):\n",
    "    import requests\n",
    "\n",
    "    url = f\"https://api.spotify.com/v1/{endpoint}\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}',\n",
    "    }\n",
    "\n",
    "    response = requests.post(url=url, headers=headers)\n",
    "    print(response)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"API Server Error - {endpoint} - Non-200 status code received: {response.status_code}\")\n",
    "    \n",
    "# INFOS <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "client_id = \"67634b8925dc48f79e59045d9e4d5014\"\n",
    "client_sc = \"b6190d9f6d404f6b86e8f0abcd1d3779\"\n",
    "user_id = \"k3u4dn9nb7cll8gtzvxb9whvt\"    \n",
    "\n",
    "# START CODE <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from math import ceil\n",
    "import json\n",
    "\n",
    "### Build Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pipeline_demo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "### Create Access Token\n",
    "access_token = get_access_token(client_id=client_id, client_sc=client_sc)\n",
    "\n",
    "### Create Playlist Lists\n",
    "endpoint = f\"users/{user_id}/playlists\"\n",
    "params = {\n",
    "    \"limit\": 50,\n",
    "    \"offset\": 0\n",
    "}\n",
    "\n",
    "playlists = get_response(access_token=access_token, endpoint=endpoint, params=params)\n",
    "json_string  = json.dumps(playlists)\n",
    "json_rdd = spark.sparkContext.parallelize([json_string])\n",
    "df_plinfo = spark.read.json(json_rdd, multiLine=True)\n",
    "\n",
    "items = df_plinfo \\\n",
    "    .withColumn(\"items\", explode(\"items\")) \\\n",
    "    .select(\"items.id\") \\\n",
    "    .rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "### Create Playlist Item Lists\n",
    "track_list = [] # <---------- \"Need To Use\"\n",
    "for id in items:\n",
    "    endpoint = f\"playlists/{id}/tracks\"\n",
    "    playlist_spec = get_response(access_token=access_token, endpoint=endpoint)\n",
    "    \n",
    "    json_string  = json.dumps(playlist_spec)\n",
    "    json_rdd = spark.sparkContext.parallelize([json_string])\n",
    "    df_playlist_spec = spark.read.json(json_rdd, multiLine=True)\n",
    "    \n",
    "    ids = df_playlist_spec \\\n",
    "    .withColumn(\"items\", explode(\"items\")) \\\n",
    "    .select(\"items.track.id\") \\\n",
    "    .rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    track_list += ids\n",
    "    \n",
    "    total = df_playlist_spec.select(\"total\").first()[0]\n",
    "    left = int(total)-100\n",
    "    cnt = ceil(left/100)\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        offset = 100 + 100 * i\n",
    "        params = {\"offset\":offset}\n",
    "        \n",
    "        playlist_spec = get_response(access_token=access_token, endpoint=endpoint, params=params)\n",
    "        \n",
    "        json_string  = json.dumps(playlist_spec)\n",
    "        json_rdd = spark.sparkContext.parallelize([json_string])\n",
    "        df_playlist_spec = spark.read.json(json_rdd, multiLine=True)\n",
    "        \n",
    "        ids = df_playlist_spec \\\n",
    "        .withColumn(\"items\", explode(\"items\")) \\\n",
    "        .select(\"items.track.id\") \\\n",
    "        .rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "        track_list += ids      \n",
    "\n",
    "cnt = ceil(len(track_list)/50)\n",
    "\n",
    "big_list = []\n",
    "for j in range(cnt):\n",
    "    big_list.append(track_list[j*50:(j+1)*50])\n",
    "\n",
    "# Create Dataframe : main_df\n",
    "main_df = None\n",
    "cnt = 0\n",
    "for small_list in big_list:\n",
    "    \n",
    "    print(cnt)\n",
    "    \n",
    "    tracks = \"\"\n",
    "    for id in small_list:\n",
    "        tracks += f\",{id}\"\n",
    "    tracks = tracks[1:]\n",
    "    \n",
    "    endpoint = \"tracks\"\n",
    "    params = {\"ids\":tracks}\n",
    "    track = get_response(access_token=access_token, endpoint=endpoint, params=params)\n",
    "    \n",
    "    json_string  = json.dumps(track)\n",
    "    json_rdd = spark.sparkContext.parallelize([json_string])\n",
    "    df_tracks = spark.read.json(json_rdd, multiLine=True)\n",
    "    \n",
    "    df_tracks = spark.read.json(json_rdd, multiLine=True) \\\n",
    "        .withColumn(\"tracks\", explode(\"tracks\")) \\\n",
    "        .selectExpr(\"tracks.id\",\n",
    "                    \"tracks.popularity\")\n",
    "    \n",
    "    endpoint = \"audio-features\"\n",
    "    params = {\"ids\":tracks}\n",
    "    audio_features = get_response(access_token=access_token, endpoint=endpoint, params=params)\n",
    "    \n",
    "    json_string  = json.dumps(audio_features)\n",
    "    json_rdd = spark.sparkContext.parallelize([json_string])\n",
    "    df_audio_features = spark.read.json(json_rdd, multiLine=True) \\\n",
    "        .withColumn(\"audio_features\", explode(\"audio_features\")) \\\n",
    "        .selectExpr(\"audio_features.id\",\n",
    "                    \"audio_features.key\",\n",
    "                    \"audio_features.mode\",\n",
    "                    \"audio_features.time_signature\",\n",
    "                    \"audio_features.tempo\",\n",
    "                    \"audio_features.acousticness\",\n",
    "                    \"audio_features.danceability\",\n",
    "                    \"audio_features.energy\",\n",
    "                    \"audio_features.instrumentalness\",\n",
    "                    \"audio_features.liveness\",\n",
    "                    \"audio_features.loudness\",\n",
    "                    \"audio_features.speechiness\",\n",
    "                    \"audio_features.valence\")\n",
    "    \n",
    "    result_track_df = df_tracks.join(df_audio_features, \"id\", \"left\")\n",
    "    if cnt == 0:\n",
    "        main_df = result_track_df\n",
    "    else:\n",
    "        main_df = main_df.union(result_track_df)\n",
    "    cnt += 1\n",
    "\n",
    "### Load Dataframe : df_dw\n",
    "dw_tracks = spark.read.parquet(\"file:///home/kjh/data/Spotify/tracks\")\n",
    "dw_audioFeatures = spark.read.parquet(\"file:///home/kjh/data/Spotify/tracks_audioFeatures\")\n",
    "df_dw = dw_tracks.join(dw_audioFeatures, \"id\", \"left\")\n",
    "\n",
    "### Union Dataframe : df\n",
    "df = df_dw.union(main_df)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    " ### Scale Dataframe : minmax_scaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "selected_features = [\"popularity\", \"key\", \"mode\", \"time_signature\", \"tempo\", \"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\", \"valence\"]\n",
    "assembler = VectorAssembler(inputCols=selected_features, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "minmax_scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "minmax_model = minmax_scaler.fit(df_assembled)\n",
    "minmax_scaled_df = minmax_model.transform(df_assembled)\n",
    "\n",
    "### Split Dataframe : train & test\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "minmax_scaled_train = minmax_scaled_df.filter(col(\"id\").isin(track_list))\n",
    "minmax_scaled_test = minmax_scaled_df.filter(~col(\"id\").isin(track_list))\n",
    "\n",
    "### Mege Datas into Group\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "kmeans = KMeans().setK(4).setSeed(1)\n",
    "model = kmeans.fit(minmax_scaled_train)\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers:\")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "df_result = model.transform(minmax_scaled_test)\n",
    "df_result.show()\n",
    "\n",
    "df_prediction = df_result.select(\"features\", \"prediction\").show()\n",
    "\n",
    "df_mean_datas = df_result.groupBy(\"prediction\").agg(\n",
    "    col(\"prediction\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kjh-4pV5HF-C",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
